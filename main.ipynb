{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CxC III - Infinite Investments Challenge**\n",
    "### By the Secret Agents - Thirandie, Deepika, Lisa, Andy\n",
    "\n",
    "Hi Judges! This is our first times ever interacting with anything related to Data Science, so we are just happy to be here and to learn more about the subject!\n",
    "\n",
    "We have seperated our notebook into a few sections:\n",
    "1. Preliminary Thoughts and Research\n",
    "2. Data Preprocessing\n",
    "3. Running the Model\n",
    "\n",
    "Our process was to first layout our data pipeline, running through all the steps earlier and getting to running the model and getting a Cross Validation score. Then, we went through all the steps again and it on the test dataset simotaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preliminary Thoughts and Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Starting Docs - Importing Relevant history.csv from downloaded location in Personal Drive\n",
    "\n",
    "import pandas as pd;\n",
    "df = pd.read_csv(\"./history.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some quick summary statistics on our history data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells us the rows and columns there are in the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned in the provided Python notebook, there is a lot more Churned customers than unchurned ones.\n",
    "df.groupby(['label']).id.agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another notable feature about this dataset is the amount of empty values there are - some columns have little to no data at all!\n",
    "pd.options.display.max_rows = 128\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Fact, there is no entry in this file without at least 1 empty column!\n",
    "df_no_null = df.dropna()\n",
    "df_no_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering all the summary statistics, we have come to the following conclusions:\n",
    "\n",
    "1. The missing values have to be dealt with, since without it there isn't much to do at all. We plan to deal with this by using the [MICE algorithm](https://medium.com/@brijesh_soni/topic-9-mice-or-multivariate-imputation-with-chain-equation-f8fd435ca91#:~:text=MICE%20stands%20for%20Multivariate%20Imputation,produce%20a%20final%20imputed%20dataset.) for numerical and date data, and to just use the mode for categorical data. (We recognize that there are a lot of missing categorical data, so the mode might not be fully accurate - however it is reasonable enough a strategy)\n",
    "2. As for the imbalanced dataset, we plan to take the advice given in the provided notebook and attempt to balance the dataset with Churn/No Churn entries before training the model. Fortunately, because Python, there does seem to be packages that handle this for us - namely [Imbalanced Learn](https://imbalanced-learn.org/stable/index.html)\n",
    "3. To further reduce variance and to combat the effects of potentially errorenous preprocessing we will employ the Random Forest Classifier, as it randomly picks between columns and rows of data to use. This should hopefully improve the model's accuracy and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0: The Types of Columns we are dealing with\n",
    "Before starting, we should replace unify all NaNs with `np.nan` for unifying reasons. We will also look at each of the columns to determine its type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "null_mask = df.isna()\n",
    "df[null_mask] = np.NaN\n",
    "\n",
    "test_null_mask = test.isna()\n",
    "test[test_null_mask] = np.NaN\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks the same, but we are assured that all the NaNs are now actually NaNs. As for the columns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine which of the columns are numerical vs categorical with the following code:\n",
    "\n",
    "# Our First Filter splits all float/integer columns from the others. However, the categorical ones are still mixed with date data etc.\n",
    "categorical_cols = [\n",
    "    col for col in df.columns\n",
    "    if df[col].dtype not in [\"float64\", \"int64\"] # removes: float, integer columns\n",
    "]\n",
    "numerical_cols = [\n",
    "    col for col in df.columns\n",
    "    if df[col].dtype in [\"float64\", \"int64\"]\n",
    "]\n",
    "\n",
    "print(\"Numerical Columns: \", numerical_cols)\n",
    "print(\"Categorical Columns: \", categorical_cols)\n",
    "\n",
    "print(\"Number of numerical columns in the DataFrame:\", len(numerical_cols))\n",
    "print(\"Number of categorical columns in the DataFrame:\", len(categorical_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nominal columns are those that have words or a few categories in them. Notably, some of these columns contain names or something similar and so would have over thousands of categories. However, since there are a few entries with a lot of one type of entry it still would be important to keep that data in... to combat this we can introduce an additional category for 'infrequent' if there are not enough entries of that type. This value will be manually determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_nominals = ['type_code', 'country_code', 'currency_code', 'debit_code', 'branch', 'retail_plan', 'language_code', 'sss_agent', 'terminal_code', 'iso_funds_code',\n",
    "                           'dup_trip_quad_code',  'special_tag', 'non_plan_book_value_flag',\n",
    "                           'risk_tolerance', 'investment_objective', 'last_maintenance_user',\n",
    "                            'retail_last_maintenance_user', 'arp_pension_origin', 'conjunction', 'loan_limit_override', 'special_fee_code']\n",
    "print(\"Number of categoricalcols_nominal in DataFrame:\", len(categorical_cols_nominals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nominal columns are those that have words or a few categories in them. Notably, some of these columns contain names or something similar and so would have over thousands of categories. However, since there are a few entries with a lot of one type of entry it still would be important to keep that data in... to combat this we can introduce an additional category for 'infrequent' if there are not enough entries of that type. This value will be manually determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_bools = ['is_registered', 'is_active', 'tms_settlement_location',\n",
    "                         'net_of_fees', 'fee_paid_separately', 'custody_fee_withdrawal',\n",
    "                         'is_fee_exempt', 'include_client_consolidation', 'use_client_address',\n",
    "                         'is_spousal',  'is_arp_locked', 'sss_location', 'sss_type', 'use_hand_delivery',\n",
    "                          'use_mail', 'share_name_address_to_issuer', 'shareholder_instructions_received',\n",
    "                          'rrsp_limit_reached',  'is_portfolio_account', 'has_no_min_commission',\n",
    "                          'is_tms_eligible', 'is_agent_bbs_participant', 'is_parameters_account',\n",
    "                          'is_spousal_transfer', 'spousal_age_flag', 'has_multiple_name',\n",
    "                          'discretionary_trading_authorized', 'shareholder_language', 'title',\n",
    "                          'function_code',  'receive_general_mailings', 'has_discrete_auth',\n",
    "                          'is_non_objecting_beneficial_owner', 'is_objecting_to_disclose_info',\n",
    "                          'consent_to_pay_for_mail', 'consent_to_email_delivery',\n",
    "                          'has_received_instruction', 'is_broker_account',\n",
    "                          'is_inventory_account', 'is_gl_account', 'is_control_account',\n",
    "                          'is_extract_eligible', 'is_pledged', 'is_resp',\n",
    "                          'use_original_date_for_payment_calc',  'is_family_resp',\n",
    "                          'is_hrdc_resp',  'is_plan_grandfathered', 'is_olob', 'visible_in_reports', 'is_midwest_clearing_account']\n",
    "print(\"Number of categoricalcols_bools in DataFrame:\", len(categorical_cols_bools))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean columns are those that are simple true and falses, which is nice! Of course the representation of T/F apparently differs but that is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols_date = ['last_trade_date', 'inception_date', 'last_update_date', 'last_maintenance_time',  'non_calendar_year_end',\n",
    "                         'plan_effective_date', 'plan_end_date',  'rrif_original_date',\n",
    "                        'inserted_at',  'updated_at', 'retail_last_maintenance_time' ]\n",
    "\n",
    "print(\"Number of categoricalcols_date in DataFrame:\", len(categorical_cols_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These Categorical Columns are actually Date columns, so we need to process them at a later date. From the names, it seems that turning them into numbers based off today's time would work as a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = ['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we have our label column, which is the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Converting the Date Columns\n",
    "Let's hope Python's Date Libraries are strong enough so that we don't have to put too much effort into converting them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols_date:\n",
    "  # Convert date string into date object, then subtract today from it and get the seconds in between. If it can't be parsed right just ignore it\n",
    "  # The hardest part about this was figuring out the timezone specifics but otherwise not bad\n",
    "  df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
    "  df[col] = (df[col] - pd.Timestamp.now().tz_localize('UTC')).dt.total_seconds()\n",
    "\n",
    "  test[col] = pd.to_datetime(test[col], errors='coerce', utc=True)\n",
    "  test[col] = (test[col] - pd.Timestamp.now().tz_localize('UTC')).dt.total_seconds()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Imputing the Non-numerical Columns\n",
    "We are now ready to impute all missing values. To do this, we use the mode for nominal/boolean ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Simple Imputer for those Categorical items\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "nominalToImpute = df[categorical_cols_nominals]\n",
    "test_nominalToImpute = test[categorical_cols_nominals]\n",
    "\n",
    "Imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "Imputer.set_output(transform=\"pandas\")\n",
    "\n",
    "nominalToImpute = Imputer.fit_transform(nominalToImpute)\n",
    "test_nominalToImpute = Imputer.transform(test_nominalToImpute)\n",
    "nominalToImpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "boolsToImpute = df[categorical_cols_bools]\n",
    "test_boolsToImpute = test[categorical_cols_bools]\n",
    "\n",
    "boolImputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "boolImputer.set_output(transform=\"pandas\")\n",
    "\n",
    "boolsToImpute = boolImputer.fit_transform(boolsToImpute)\n",
    "test_boolsToImpute = boolImputer.transform(test_boolsToImpute)\n",
    "boolsToImpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge them back in\n",
    "df[categorical_cols_nominals] = nominalToImpute\n",
    "df[categorical_cols_bools] = boolsToImpute\n",
    "\n",
    "test[categorical_cols_nominals] = test_nominalToImpute\n",
    "test[categorical_cols_bools] = test_boolsToImpute\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: One-Hot Encoding\n",
    "Before imputing numerical values, we first one-hot encode everything else. However, as mentioned earlier, as some of the fields contain literally thousands of values we implement a method to get rid of categories that are not substantial.\n",
    "\n",
    "Of course, since we backfilled our categorical data with the mode already, we need to be careful with what the cutoff should be. Manual checking from earlier suggests that maybe, uh, **500**? Also, we have to cap the max categories to 20 - otherwise we get like 1000 or so columns, which isn't great. Just so you know, 20 was deliberately chosen because of `type_code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "nominalOneHot = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='infrequent_if_exist', min_frequency=500, max_categories=20)\n",
    "nominalOneHot.set_output(transform=\"pandas\")\n",
    "\n",
    "nomOneHot = df[categorical_cols_nominals].astype(str)\n",
    "nomOneHot = nominalOneHot.fit_transform(nomOneHot)\n",
    "\n",
    "test_nomOneHot = df[categorical_cols_nominals].astype(str)\n",
    "test_nomOneHot = nominalOneHot.transform(test_nomOneHot)\n",
    "\n",
    "nomOneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "booleanOneHot = OneHotEncoder(sparse_output=False, drop='if_binary', handle_unknown='infrequent_if_exist', min_frequency=500)\n",
    "booleanOneHot.set_output(transform=\"pandas\")\n",
    "\n",
    "boolOneHot = df[categorical_cols_bools].astype(str)\n",
    "boolOneHot = booleanOneHot.fit_transform(boolOneHot)\n",
    "\n",
    "test_boolOneHot = df[categorical_cols_bools].astype(str)\n",
    "test_boolOneHot = booleanOneHot.transform(test_boolOneHot)\n",
    "\n",
    "boolOneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nomOneHot.columns)\n",
    "print(boolOneHot.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge them back in\n",
    "df = pd.concat([df, nomOneHot, boolOneHot], axis=1)\n",
    "df = df.drop(categorical_cols_nominals, axis=1).drop(categorical_cols_bools, axis=1)\n",
    "\n",
    "test = pd.concat([test, test_nomOneHot, test_boolOneHot], axis=1)\n",
    "test = test.drop(categorical_cols_nominals, axis=1).drop(categorical_cols_bools, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Savepoint**\n",
    "To Reduce on RAM (and because when we did this on Google Colab it ran out of RAM) ((Free Powerful Cloud Computing is a lie))(((literally it was faster to run these operations locally on a 7 year old laptop))) we will be making a quicksave of the data here - we still need to MICE it, which is a very labourious operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./history-onehot.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "df2 = pd.read_csv(\"./history-onehot.csv\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: MICE Imputing\n",
    "Now for the rest of the rows. MICE time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to cut off the label, since that isn't a number and MICE cannot handle it.\n",
    "df_X = df2.drop(['label'], axis=1)\n",
    "df_Y = df2['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently its still a experimental feature within sklearn - but that won't stop us!\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "total_num_cols = np.concatenate((numerical_cols, categorical_cols_date))\n",
    "\n",
    "numImpute = IterativeImputer(random_state=23950, initial_strategy='median')\n",
    "numImpute.set_output(transform=\"pandas\")\n",
    "\n",
    "toNumImpute = df_X[total_num_cols]\n",
    "toNumImpute = numImpute.fit_transform(toNumImpute)\n",
    "toNumImpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge it in\n",
    "df_X = df_X.drop(total_num_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = pd.concat([df_X, toNumImpute], axis=1)\n",
    "df_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should be all of the nulls taken care of! Hopefully this didn't corrupt our data too much..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df_X, df_Y], axis=1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Encode the Y-Label as Well\n",
    "Almost forgot - gotta label encode the Churn/No Churn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df3['label'] = label_encoder.fit_transform(df3['label'])\n",
    "\n",
    "print(label_encoder.classes_)\n",
    "\n",
    "# I know we basically already did this but just to be sure you know\n",
    "X = df3.drop([\"label\"], axis=1)\n",
    "y = df3[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Savepoint**\n",
    "It's important to save!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('./history_X.csv')\n",
    "y.to_csv('./history_y.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Split training / testing data\n",
    "Hopefully this is good! Going to follow the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Oops, the X and y has an extra column I forgot to get rid of my bad\n",
    "X = pd.read_csv('./history_X.csv')\n",
    "X = X.drop(X.columns[0], axis=1)\n",
    "y = pd.read_csv('./history_y.csv')\n",
    "y = y.drop(y.columns[0], axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: The Model\n",
    "Its just the Bagged Random Forest Classifier that was taken off `imblearn` - just gonna quickly, uh, steal the provided code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "classifer = BalancedRandomForestClassifier(max_features='log2', bootstrap=True, verbose=1, random_state=13, replacement=True)\n",
    "cv_scores = cross_val_score(estimator=classifer, X=X_train, y=y_train, cv=5)\n",
    "print(f\"Average CV Score: {sum(cv_scores)/len(cv_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "classifer.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifer.predict(X_val)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah thats pretty good! Now to copy this onto the `test.csv`. We employ the same preprocessing steps, and then get the `classifer` (which we've just noticed is misspelt) to predict on the new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Doing it all again but on the `test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_null_mask = test.isna()\n",
    "test[test_null_mask] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols_date:\n",
    "  # Convert date string into date object, then subtract today from it and get the seconds in between. If it can't be parsed right just ignore it\n",
    "  # The hardest part about this was figuring out the timezone specifics but otherwise not bad\n",
    "  test[col] = pd.to_datetime(test[col], errors='coerce', utc=True)\n",
    "  test[col] = (test[col] - pd.Timestamp.now().tz_localize('UTC')).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
